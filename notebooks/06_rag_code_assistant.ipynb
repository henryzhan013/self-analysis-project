{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# RAG System for Code Repository Queries\n",
    "\n",
    "This notebook demonstrates a **Retrieval-Augmented Generation (RAG)** system that allows natural language queries about your GitHub repositories.\n",
    "\n",
    "**How it works:**\n",
    "1. **Indexing**: Embed repository metadata, READMEs, and commit messages using `nomic-embed-text`\n",
    "2. **Retrieval**: Find the most relevant chunks using cosine similarity\n",
    "3. **Generation**: Use an LLM to generate answers based on retrieved context\n",
    "\n",
    "**Components:**\n",
    "- Embedding model: `nomic-embed-text` (via Ollama)\n",
    "- Vector store: Simple numpy-based cosine similarity\n",
    "- LLM: `llama3.1:8b` (via Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:47:20.896716Z",
     "iopub.status.busy": "2026-02-14T21:47:20.896598Z",
     "iopub.status.idle": "2026-02-14T21:47:21.314406Z",
     "shell.execute_reply": "2026-02-14T21:47:21.314132Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src.rag.code_rag import CodeRAG\n",
    "\n",
    "DATA = Path(\"..\") / \"data\"\n",
    "OUT = Path(\"..\") / \"outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:47:21.315989Z",
     "iopub.status.busy": "2026-02-14T21:47:21.315865Z",
     "iopub.status.idle": "2026-02-14T21:47:21.324849Z",
     "shell.execute_reply": "2026-02-14T21:47:21.324623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repositories: 7\n",
      "READMEs: 5\n",
      "Commits: 23\n",
      "\n",
      "Repos: ['Competitive-Programming', 'leetcode-solutions', 'nfl-bdb-2024', 'polars', 'questions', 'roadtrip-planner', 'roadtripplanner']\n"
     ]
    }
   ],
   "source": [
    "repos = pd.read_csv(DATA / \"repos.csv\")\n",
    "readmes = pd.read_csv(DATA / \"readmes.csv\")\n",
    "languages = pd.read_csv(DATA / \"languages.csv\")\n",
    "commits = pd.read_csv(DATA / \"commits.csv\")\n",
    "\n",
    "print(f\"Repositories: {len(repos)}\")\n",
    "print(f\"READMEs: {len(readmes)}\")\n",
    "print(f\"Commits: {len(commits)}\")\n",
    "print(f\"\\nRepos: {repos['repo_name'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "index-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Initialize RAG System & Index Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "init-rag",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:47:21.326088Z",
     "iopub.status.busy": "2026-02-14T21:47:21.326011Z",
     "iopub.status.idle": "2026-02-14T21:47:22.770194Z",
     "shell.execute_reply": "2026-02-14T21:47:22.769585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing repositories...\n",
      "  Embedding: overview for Competitive-Programming...\n",
      "  Embedding: commits for Competitive-Programming...\n",
      "  Embedding: overview for leetcode-solutions...\n",
      "  Embedding: readme for leetcode-solutions...\n",
      "  Embedding: commits for leetcode-solutions...\n",
      "  Embedding: overview for nfl-bdb-2024...\n",
      "  Embedding: readme for nfl-bdb-2024...\n",
      "  Embedding: overview for polars...\n",
      "  Embedding: readme for polars...\n",
      "  Embedding: commits for polars...\n",
      "  Embedding: overview for questions...\n",
      "  Embedding: overview for roadtrip-planner...\n",
      "  Embedding: readme for roadtrip-planner...\n",
      "  Embedding: commits for roadtrip-planner...\n",
      "  Embedding: overview for roadtripplanner...\n",
      "  Embedding: readme for roadtripplanner...\n",
      "  Embedding: commits for roadtripplanner...\n",
      "Indexed 17 chunks from 7 repositories.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG system\n",
    "rag = CodeRAG(\n",
    "    embedding_model=\"nomic-embed-text\",\n",
    "    llm_model=\"llama3.1:8b\"\n",
    ")\n",
    "\n",
    "# Index all repositories\n",
    "rag.index_repositories(repos, readmes, languages, commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "save-index",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:47:22.774059Z",
     "iopub.status.busy": "2026-02-14T21:47:22.773592Z",
     "iopub.status.idle": "2026-02-14T21:47:22.779595Z",
     "shell.execute_reply": "2026-02-14T21:47:22.779060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved RAG index to ../outputs/rag_index\n"
     ]
    }
   ],
   "source": [
    "# Save the index for future use\n",
    "rag.save(OUT / \"rag_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Query the RAG System\n",
    "\n",
    "Now we can ask natural language questions about the codebase!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "query1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:47:22.781887Z",
     "iopub.status.busy": "2026-02-14T21:47:22.781710Z",
     "iopub.status.idle": "2026-02-14T21:47:32.762714Z",
     "shell.execute_reply": "2026-02-14T21:47:32.761948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What programming languages do I use most?\n",
      "\n",
      "ANSWER:\n",
      "The context does not provide sufficient information about the programming languages used in the repository \"Competitive-Programming\". The \"Primary Language\" is listed as \"nan\", which is likely an abbreviation for \"not available\" or \"unknown\", and there are no other language-related details provided. Therefore, I cannot determine what programming languages you use most based on this context.\n",
      "\n",
      "Sources: [{'repo': 'Competitive-Programming', 'type': 'overview', 'similarity': 0.563}, {'repo': 'polars', 'type': 'commits', 'similarity': 0.53}, {'repo': 'leetcode-solutions', 'type': 'readme', 'similarity': 0.528}]\n",
      "Latency: 9.98s\n"
     ]
    }
   ],
   "source": [
    "# Query 1: General question about projects\n",
    "result = rag.query(\"What programming languages do I use most?\")\n",
    "\n",
    "print(\"QUESTION:\", result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(f\"\\nSources: {result['sources']}\")\n",
    "print(f\"Latency: {result['latency_s']}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "query2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:47:32.774969Z",
     "iopub.status.busy": "2026-02-14T21:47:32.774766Z",
     "iopub.status.idle": "2026-02-14T21:47:44.901683Z",
     "shell.execute_reply": "2026-02-14T21:47:44.901018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What is the polars project about?\n",
      "\n",
      "ANSWER:\n",
      "The polars project is an analytical query engine written for DataFrames, designed to be fast, easy to use, and expressive. It supports various programming languages including Python, Rust, Node.js, R, and SQL, and uses Apache Arrow Columnar Format. The project aims to provide a powerful expression API with features like lazy or eager execution, streaming, query optimization, multi-threading, SIMD, and more.\n",
      "\n",
      "Sources: [{'repo': 'polars', 'type': 'overview', 'similarity': 0.684}, {'repo': 'polars', 'type': 'readme', 'similarity': 0.669}, {'repo': 'polars', 'type': 'commits', 'similarity': 0.598}]\n",
      "Latency: 12.12s\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Specific project question\n",
    "result = rag.query(\"What is the polars project about?\")\n",
    "\n",
    "print(\"QUESTION:\", result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(f\"\\nSources: {result['sources']}\")\n",
    "print(f\"Latency: {result['latency_s']}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "query3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:47:44.904352Z",
     "iopub.status.busy": "2026-02-14T21:47:44.904181Z",
     "iopub.status.idle": "2026-02-14T21:47:49.716115Z",
     "shell.execute_reply": "2026-02-14T21:47:49.715515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What kind of commits have I been making recently?\n",
      "\n",
      "ANSWER:\n",
      "Based on the provided context, it appears that you've been creating new problems for Competitive-Programming (e.g., \"Create 1772E\", \"Create 1738C\"), adding new solutions for leetcode-solutions (e.g., \"add new stuff\", \"new\"), and making initial commits for roadtripplanner.\n",
      "\n",
      "Sources: [{'repo': 'Competitive-Programming', 'type': 'commits', 'similarity': 0.742}, {'repo': 'leetcode-solutions', 'type': 'commits', 'similarity': 0.691}, {'repo': 'roadtripplanner', 'type': 'commits', 'similarity': 0.676}]\n",
      "Latency: 4.81s\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Activity patterns\n",
    "result = rag.query(\"What kind of commits have I been making recently?\")\n",
    "\n",
    "print(\"QUESTION:\", result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(f\"\\nSources: {result['sources']}\")\n",
    "print(f\"Latency: {result['latency_s']}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "query4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:47:49.719025Z",
     "iopub.status.busy": "2026-02-14T21:47:49.718784Z",
     "iopub.status.idle": "2026-02-14T21:47:58.760679Z",
     "shell.execute_reply": "2026-02-14T21:47:58.760063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Based on my projects, what are my technical skills?\n",
      "\n",
      "ANSWER:\n",
      "Based on the provided context, it appears that you have experience with Polars, which is a Rust library for parallel computing and data manipulation. The recent commits suggest that you are familiar with:\n",
      "\n",
      "1. Git (based on the commit messages)\n",
      "2. Rust programming language\n",
      "3. Polars library specifically (with commits related to its functionality)\n",
      "\n",
      "However, there is no information about your technical skills in the context provided. The README for roadtrip-planner and the repository Competitive-Programming do not contain any relevant information about your technical skills.\n",
      "\n",
      "To answer your question more accurately, I would need additional context or information about your projects and skills.\n",
      "\n",
      "Sources: [{'repo': 'polars', 'type': 'commits', 'similarity': 0.535}, {'repo': 'Competitive-Programming', 'type': 'overview', 'similarity': 0.519}, {'repo': 'roadtrip-planner', 'type': 'readme', 'similarity': 0.518}]\n",
      "Latency: 9.04s\n"
     ]
    }
   ],
   "source": [
    "# Query 4: Skills question\n",
    "result = rag.query(\"Based on my projects, what are my technical skills?\")\n",
    "\n",
    "print(\"QUESTION:\", result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(f\"\\nSources: {result['sources']}\")\n",
    "print(f\"Latency: {result['latency_s']}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "query5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:47:58.766229Z",
     "iopub.status.busy": "2026-02-14T21:47:58.766042Z",
     "iopub.status.idle": "2026-02-14T21:48:11.285288Z",
     "shell.execute_reply": "2026-02-14T21:48:11.284795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Which of my projects would be most impressive to show an employer?\n",
      "\n",
      "ANSWER:\n",
      "Based on the provided context, I would recommend showing the \"roadtrip-planner\" project to an employer as it appears to be a more substantial and complex project compared to the other two repositories.\n",
      "\n",
      "The roadtrip-planner repository has multiple commits with significant changes, including setting up a React frontend with search functionality, adding a FastAPI backend, and implementing a vibe-based place search pipeline. It also includes several Python scripts for collecting places from Google Places API and generating semantic embeddings using sentence-transformers.\n",
      "\n",
      "In contrast, the \"Competitive-Programming\" repository appears to be a collection of solutions to competitive programming problems, which may not demonstrate as much technical expertise or project management skills as the roadtrip-planner project. The leetcode-solutions repository is simply a README file with no additional information about the project.\n",
      "\n",
      "Therefore, showcasing the roadtrip-planner project would likely be more impressive to an employer due to its complexity and scope.\n",
      "\n",
      "Sources: [{'repo': 'roadtrip-planner', 'type': 'commits', 'similarity': 0.484}, {'repo': 'Competitive-Programming', 'type': 'overview', 'similarity': 0.473}, {'repo': 'leetcode-solutions', 'type': 'readme', 'similarity': 0.471}]\n",
      "Latency: 12.52s\n"
     ]
    }
   ],
   "source": [
    "# Query 5: Recommendation question\n",
    "result = rag.query(\"Which of my projects would be most impressive to show an employer?\")\n",
    "\n",
    "print(\"QUESTION:\", result[\"question\"])\n",
    "print(\"\\nANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(f\"\\nSources: {result['sources']}\")\n",
    "print(f\"Latency: {result['latency_s']}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Interactive Query Function\n",
    "\n",
    "Use this function to ask your own questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interactive",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:48:11.287648Z",
     "iopub.status.busy": "2026-02-14T21:48:11.287484Z",
     "iopub.status.idle": "2026-02-14T21:48:24.430139Z",
     "shell.execute_reply": "2026-02-14T21:48:24.429666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Do I have any experience with data processing?\n",
      "\n",
      "A: Based on the provided context, there is no direct indication that you have experience with data processing. However, the README for the nfl-bdb-2024 repository does mention \"curated datasets\" and \"reproducible figures\", which suggests some level of data processing and analysis has been done in this specific project.\n",
      "\n",
      "It's also worth noting that the repository includes a notebook called `01_analysis.ipynb` under the `notebooks/` directory, which might imply that some form of data analysis is being performed. However, without more information about your personal experience or involvement with this project, it's difficult to say for certain whether you have experience with data processing.\n",
      "\n",
      "If I had to provide a tentative answer based on the context provided, I would say \"it appears that someone involved in this project has some level of experience with data processing, but it's unclear if that person is you.\"\n",
      "\n",
      "[Sources: ['nfl-bdb-2024/readme', 'questions/overview', 'Competitive-Programming/overview'] | 13.14s]\n"
     ]
    }
   ],
   "source": [
    "def ask(question: str):\n",
    "    \"\"\"Ask a question about your repositories.\"\"\"\n",
    "    result = rag.query(question)\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"\\nA: {result['answer']}\")\n",
    "    print(f\"\\n[Sources: {[s['repo'] + '/' + s['type'] for s in result['sources']]} | {result['latency_s']}s]\")\n",
    "\n",
    "# Example usage:\n",
    "ask(\"Do I have any experience with data processing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "---\n",
    "## RAG System Evaluation\n",
    "\n",
    "Let's evaluate the RAG system's performance across multiple queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "evaluation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:48:24.432459Z",
     "iopub.status.busy": "2026-02-14T21:48:24.432312Z",
     "iopub.status.idle": "2026-02-14T21:48:55.433212Z",
     "shell.execute_reply": "2026-02-14T21:48:55.432724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: What is the largest repository?...\n",
      "Processed: Which repos are forks?...\n",
      "Processed: What languages do I know?...\n",
      "Processed: Describe my Python projects...\n",
      "Processed: What kind of developer am I?...\n",
      "\n",
      "============================================================\n",
      "RAG EVALUATION RESULTS\n",
      "============================================================\n",
      "                       question  answer_length              top_source  top_similarity  latency_s\n",
      "What is the largest repository?            440            nfl-bdb-2024           0.551       8.42\n",
      "         Which repos are forks?            364        roadtrip-planner           0.513       6.32\n",
      "      What languages do I know?            249               questions           0.543       4.06\n",
      "    Describe my Python projects            431 Competitive-Programming           0.556       5.40\n",
      "   What kind of developer am I?            492                  polars           0.568       6.79\n",
      "\n",
      "Average latency: 6.20s\n",
      "Average similarity: 0.546\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with a set of test queries\n",
    "test_queries = [\n",
    "    \"What is the largest repository?\",\n",
    "    \"Which repos are forks?\",\n",
    "    \"What languages do I know?\",\n",
    "    \"Describe my Python projects\",\n",
    "    \"What kind of developer am I?\"\n",
    "]\n",
    "\n",
    "eval_results = []\n",
    "for q in test_queries:\n",
    "    result = rag.query(q)\n",
    "    eval_results.append({\n",
    "        \"question\": q,\n",
    "        \"answer_length\": len(result[\"answer\"]),\n",
    "        \"top_source\": result[\"sources\"][0][\"repo\"] if result[\"sources\"] else \"N/A\",\n",
    "        \"top_similarity\": result[\"sources\"][0][\"similarity\"] if result[\"sources\"] else 0,\n",
    "        \"latency_s\": result[\"latency_s\"]\n",
    "    })\n",
    "    print(f\"Processed: {q[:40]}...\")\n",
    "\n",
    "eval_df = pd.DataFrame(eval_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(eval_df.to_string(index=False))\n",
    "print(f\"\\nAverage latency: {eval_df['latency_s'].mean():.2f}s\")\n",
    "print(f\"Average similarity: {eval_df['top_similarity'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This RAG system demonstrates:\n",
    "\n",
    "1. **Semantic Search**: Using embeddings to find relevant content based on meaning, not just keywords\n",
    "2. **Context-Aware Generation**: LLM answers are grounded in actual repository data\n",
    "3. **Source Attribution**: Each answer cites which repositories and document types were used\n",
    "\n",
    "**Potential Improvements:**\n",
    "- Add more granular chunking (function-level, file-level)\n",
    "- Include actual source code content\n",
    "- Implement hybrid search (semantic + keyword)\n",
    "- Add reranking for better retrieval accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "final-stats",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T21:48:55.436112Z",
     "iopub.status.busy": "2026-02-14T21:48:55.435943Z",
     "iopub.status.idle": "2026-02-14T21:48:55.439491Z",
     "shell.execute_reply": "2026-02-14T21:48:55.439166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAG SYSTEM STATISTICS\n",
      "============================================================\n",
      "Total indexed chunks: 17\n",
      "Embedding model: nomic-embed-text\n",
      "LLM model: llama3.1:8b\n",
      "Vector dimensions: 768\n",
      "\n",
      "Chunks by type:\n",
      "  readme: 5\n",
      "  overview: 7\n",
      "  commits: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RAG SYSTEM STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total indexed chunks: {len(rag.vector_store.documents)}\")\n",
    "print(f\"Embedding model: nomic-embed-text\")\n",
    "print(f\"LLM model: llama3.1:8b\")\n",
    "print(f\"Vector dimensions: {rag.vector_store.embeddings.shape[1]}\")\n",
    "print(f\"\\nChunks by type:\")\n",
    "for doc_type in set(d['type'] for d in rag.vector_store.documents):\n",
    "    count = sum(1 for d in rag.vector_store.documents if d['type'] == doc_type)\n",
    "    print(f\"  {doc_type}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
